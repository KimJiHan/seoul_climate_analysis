{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Introduction to Seoul Heatwave Analysis\n",
    "## Environment Setup and Course Overview\n",
    "\n",
    "**Instructor**: Sohn Chul\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "1. Understand the importance of heatwave analysis in urban environments\n",
    "2. Set up Python environment for climate data analysis\n",
    "3. Navigate the S-DoT sensor network data structure\n",
    "4. Execute basic data loading and exploration tasks\n",
    "5. Use Git/GitHub for version control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Background\n",
    "\n",
    "### 1.1 Why Study Urban Heatwaves?\n",
    "\n",
    "Urban heatwaves pose significant challenges:\n",
    "- **Public Health**: Increased mortality and morbidity\n",
    "- **Energy Demand**: Peak cooling loads strain infrastructure\n",
    "- **Economic Impact**: Reduced productivity and increased healthcare costs\n",
    "- **Environmental Justice**: Disproportionate impacts on vulnerable populations\n",
    "\n",
    "### 1.2 Seoul's Climate Context\n",
    "\n",
    "Seoul experiences:\n",
    "- Hot, humid summers (June-August)\n",
    "- Urban Heat Island (UHI) effects\n",
    "- Rapid urbanization increasing heat vulnerability\n",
    "- Need for data-driven mitigation strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. S-DoT Sensor Network\n",
    "\n",
    "### 2.1 What is S-DoT?\n",
    "\n",
    "**S-DoT (Smart Seoul Data of Things)** is Seoul's IoT sensor network:\n",
    "- Over 1,100 sensors across the city\n",
    "- Measures environmental parameters every 10 minutes\n",
    "- Real-time data for urban management\n",
    "\n",
    "### 2.2 Data Variables\n",
    "\n",
    "| Variable | Unit | Description |\n",
    "|----------|------|-------------|\n",
    "| Temperature | ¬∞C | Air temperature |\n",
    "| Humidity | % | Relative humidity |\n",
    "| PM2.5 | Œºg/m¬≥ | Fine particulate matter |\n",
    "| PM10 | Œºg/m¬≥ | Coarse particulate matter |\n",
    "| Noise | dB | Sound level |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Environment Setup\n",
    "\n",
    "### 3.1 Check Python Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python path: {sys.executable}\")\n",
    "\n",
    "# Should be Python 3.8 or higher\n",
    "assert sys.version_info >= (3, 8), \"Python 3.8 or higher required\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# üåç Environment-specific Package Installation\nimport sys\nimport os  # os Î™®Îìà import Ï∂îÍ∞Ä\n\ndef install_required_packages():\n    \"\"\"\n    Install required packages based on the environment\n    \"\"\"\n    IN_COLAB = 'google.colab' in sys.modules\n    IN_KAGGLE = 'kaggle_secrets' in sys.modules or 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n    \n    if IN_COLAB:\n        print(\"üì± Installing packages for Google Colab...\")\n        \n        # Note: In Jupyter, use !pip for shell commands\n        # Colab-specific installations\n        # !pip install -q geopandas folium plotly\n        # !pip install -q scikit-learn statsmodels prophet xgboost\n        # !pip install -q tqdm python-dotenv openpyxl xlrd\n        \n        print(\"üí° Run the following commands in Colab:\")\n        print(\"!pip install -q geopandas folium plotly\")\n        print(\"!pip install -q scikit-learn statsmodels prophet xgboost\")\n        print(\"!pip install -q tqdm python-dotenv openpyxl xlrd\")\n        print(\"\\n‚úÖ Then Colab packages will be installed!\")\n        \n    elif IN_KAGGLE:\n        print(\"üèÜ Kaggle environment detected - most packages pre-installed\")\n        \n        # Kaggle-specific installations (if needed)\n        try:\n            import folium\n            print(\"‚úÖ folium is already installed\")\n        except ImportError:\n            print(\"‚ùå folium not found. Run: !pip install -q folium\")\n            \n    else:\n        print(\"üíª Local environment detected\")\n        print(\"üìù Ensure you have installed packages from requirements.txt:\")\n        print(\"   pip install -r ../requirements.txt\")\n        print(\"\\nCore packages needed:\")\n        print(\"   ‚Ä¢ pandas, numpy, matplotlib, seaborn\")\n        print(\"   ‚Ä¢ geopandas, folium, plotly\")\n        print(\"   ‚Ä¢ scikit-learn, statsmodels\")\n        print(\"   ‚Ä¢ jupyter, ipywidgets, tqdm\")\n        \n        # Check installed packages\n        print(\"\\nüîç Checking installed packages...\")\n        packages_to_check = ['pandas', 'numpy', 'matplotlib', 'seaborn', \n                            'geopandas', 'folium', 'plotly', 'sklearn', 'statsmodels']\n        \n        missing_packages = []\n        for package in packages_to_check:\n            try:\n                __import__(package)\n                print(f\"  ‚úÖ {package}\")\n            except ImportError:\n                print(f\"  ‚ùå {package} - not installed\")\n                missing_packages.append(package)\n        \n        if missing_packages:\n            print(f\"\\n‚ö†Ô∏è Missing packages: {', '.join(missing_packages)}\")\n            print(\"üì¶ Install them with:\")\n            print(f\"   pip install {' '.join(missing_packages)}\")\n        else:\n            print(\"\\n‚úÖ All core packages are installed!\")\n\n# Run the function to check packages\ninstall_required_packages()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Import Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Date handling\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# File handling\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Configure Visualization Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set plotting style\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\n# Configure matplotlib for inline display\n%matplotlib inline\n\n# Set default figure size\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['font.size'] = 12\n\n# üåç Environment-specific Korean font configuration\ndef configure_korean_fonts():\n    \"\"\"\n    Configure Korean fonts based on the environment\n    \"\"\"\n    import sys\n    import platform\n    import matplotlib.pyplot as plt\n    import matplotlib.font_manager as fm\n    \n    IN_COLAB = 'google.colab' in sys.modules\n    \n    if IN_COLAB:\n        print(\"  üì± Configuring fonts for Google Colab\")\n        # Install Korean fonts in Colab\n        try:\n            import subprocess\n            import os\n            \n            # Download and install Korean font\n            subprocess.run(['apt-get', 'install', '-y', 'fonts-nanum'], check=True, capture_output=True)\n            \n            # Set font\n            plt.rcParams['font.family'] = 'NanumGothic'\n            print(\"  ‚úÖ NanumGothic font configured\")\n            \n        except Exception as e:\n            print(\"  ‚ö†Ô∏è Could not install Korean font, using default\")\n            plt.rcParams['font.family'] = 'DejaVu Sans'\n            \n    else:\n        # Local environment\n        system = platform.system()\n        print(f\"  üíª Configuring fonts for {system}\")\n        \n        if system == 'Darwin':  # macOS\n            try:\n                plt.rcParams['font.family'] = 'AppleGothic'\n                print(\"  ‚úÖ AppleGothic font configured (macOS)\")\n            except:\n                try:\n                    plt.rcParams['font.family'] = 'Arial Unicode MS'\n                    print(\"  ‚úÖ Arial Unicode MS font configured (macOS)\")\n                except:\n                    plt.rcParams['font.family'] = 'DejaVu Sans'\n                    print(\"  ‚ö†Ô∏è Using default font (macOS)\")\n                    \n        elif system == 'Windows':\n            try:\n                plt.rcParams['font.family'] = 'Malgun Gothic'\n                print(\"  ‚úÖ Malgun Gothic font configured (Windows)\")\n            except:\n                try:\n                    plt.rcParams['font.family'] = 'Microsoft YaHei'\n                    print(\"  ‚úÖ Microsoft YaHei font configured (Windows)\")\n                except:\n                    plt.rcParams['font.family'] = 'DejaVu Sans'\n                    print(\"  ‚ö†Ô∏è Using default font (Windows)\")\n                    \n        else:  # Linux\n            try:\n                plt.rcParams['font.family'] = 'NanumGothic'\n                print(\"  ‚úÖ NanumGothic font configured (Linux)\")\n            except:\n                plt.rcParams['font.family'] = 'DejaVu Sans'\n                print(\"  ‚ö†Ô∏è Using default font (Linux)\")\n    \n    # Prevent minus sign issues\n    plt.rcParams['axes.unicode_minus'] = False\n\n# Configure fonts\nconfigure_korean_fonts()\n\nprint(\"‚úÖ Visualization settings configured!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Exploration\n",
    "\n",
    "### 4.1 Check Data Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# üåç Environment Detection and Path Configuration\ndef setup_environment_paths():\n    \"\"\"\n    Automatically detect environment (Local, Colab, Kaggle) and set appropriate paths\n    \"\"\"\n    import os\n    import sys\n    \n    # Detect environment\n    IN_COLAB = 'google.colab' in sys.modules\n    IN_KAGGLE = 'kaggle_secrets' in sys.modules or os.environ.get('KAGGLE_KERNEL_RUN_TYPE')\n    \n    print(\"üîç Environment Detection:\")\n    \n    if IN_COLAB:\n        print(\"  üì± Running in Google Colab\")\n        \n        # Mount Google Drive (optional)\n        try:\n            from google.colab import drive\n            drive.mount('/content/drive')\n            \n            # Check if data exists in Drive\n            drive_data_path = '/content/drive/MyDrive/seoul_heatwave_course/data'\n            if os.path.exists(drive_data_path):\n                COURSE_DATA_PATH = '/content/drive/MyDrive/seoul_heatwave_course/data'\n                print(\"  ‚úÖ Using Google Drive data path\")\n            else:\n                # Clone from GitHub or use default\n                COURSE_DATA_PATH = '/content/seoul_heatwave_course/data'\n                print(\"  üìÅ Using Colab workspace path\")\n                \n                # GitHub clone instructions\n                print(\"  üí° To get data, run: !git clone https://github.com/KimJiHan/seoul_climate_analysis.git\")\n                \n        except Exception as e:\n            COURSE_DATA_PATH = '/content/seoul_heatwave_course/data'\n            print(\"  üìÅ Using default Colab path\")\n            \n    elif IN_KAGGLE:\n        print(\"  üèÜ Running in Kaggle\")\n        COURSE_DATA_PATH = '/kaggle/input/seoul-heatwave-data'\n        \n    else:\n        print(\"  üíª Running in Local Environment\")\n        # Local environment - use relative path\n        COURSE_DATA_PATH = '../data'\n    \n    # Create absolute paths\n    SDOT_PATH = os.path.join(COURSE_DATA_PATH, 'raw', 's-dot')\n    EXTERNAL_DATA_PATH = os.path.join(COURSE_DATA_PATH, 'external')\n    PROCESSED_DATA_PATH = os.path.join(COURSE_DATA_PATH, 'processed')\n    \n    # SGIS data paths\n    SGIS_BOUNDARIES_PATH = os.path.join(EXTERNAL_DATA_PATH, 'sgis_boundaries')\n    SGIS_STATISTICS_PATH = os.path.join(EXTERNAL_DATA_PATH, 'sgis_statistics')\n    \n    return {\n        'course_data': COURSE_DATA_PATH,\n        'sdot': SDOT_PATH,\n        'external': EXTERNAL_DATA_PATH,\n        'processed': PROCESSED_DATA_PATH,\n        'sgis_boundaries': SGIS_BOUNDARIES_PATH,\n        'sgis_statistics': SGIS_STATISTICS_PATH,\n        'environment': 'colab' if IN_COLAB else 'kaggle' if IN_KAGGLE else 'local'\n    }\n\n# Setup paths\npaths = setup_environment_paths()\nCOURSE_DATA_PATH = paths['course_data']\nSDOT_PATH = paths['sdot']\nEXTERNAL_DATA_PATH = paths['external']\nPROCESSED_DATA_PATH = paths['processed']\nSGIS_BOUNDARIES_PATH = paths['sgis_boundaries']\nSGIS_STATISTICS_PATH = paths['sgis_statistics']\n\nprint(f\"\\nüìÅ Course data path: {os.path.abspath(COURSE_DATA_PATH)}\")\nprint(f\"üìä S-DoT data path: {os.path.abspath(SDOT_PATH)}\")\nprint(f\"üìã External data path: {os.path.abspath(EXTERNAL_DATA_PATH)}\")\n\n# Check if S-DoT data directory exists\nif os.path.exists(SDOT_PATH):\n    print(f\"\\n‚úÖ S-DoT data directory found: {SDOT_PATH}\")\n    \n    # List CSV files\n    csv_files = glob.glob(os.path.join(SDOT_PATH, '*.csv'))\n    total_size = sum(os.path.getsize(f) for f in csv_files) / (1024 * 1024 * 1024)  # GB\n    \n    print(f\"üìä Found {len(csv_files)} CSV files (Total: {total_size:.2f} GB)\")\n    print(\"üìù Sample files:\")\n    \n    for file in sorted(csv_files)[:5]:  # Show first 5 files\n        file_size = os.path.getsize(file) / (1024 * 1024)  # Convert to MB\n        print(f\"  ‚Ä¢ {os.path.basename(file)} ({file_size:.1f} MB)\")\n    \n    if len(csv_files) > 5:\n        print(f\"  ... and {len(csv_files) - 5} more files\")\n        \n    # Check external data\n    if os.path.exists(EXTERNAL_DATA_PATH):\n        print(f\"\\nüìã External data files found:\")\n        \n        # Check Excel files\n        excel_files = glob.glob(os.path.join(EXTERNAL_DATA_PATH, '*.xlsx'))\n        for file in excel_files:\n            file_size = os.path.getsize(file) / 1024  # KB\n            print(f\"  ‚Ä¢ {os.path.basename(file)} ({file_size:.0f} KB)\")\n        \n        # Check SGIS boundaries\n        if os.path.exists(SGIS_BOUNDARIES_PATH):\n            print(f\"  ‚Ä¢ SGIS Administrative Boundaries:\")\n            boundary_dirs = [d for d in os.listdir(SGIS_BOUNDARIES_PATH) \n                           if os.path.isdir(os.path.join(SGIS_BOUNDARIES_PATH, d)) and not d.startswith('.')]\n            for boundary_dir in boundary_dirs:\n                print(f\"    - {boundary_dir}\")\n        \n        # Check SGIS statistics\n        if os.path.exists(SGIS_STATISTICS_PATH):\n            print(f\"  ‚Ä¢ SGIS Statistics Data: ‚úÖ\")\n    \nelse:\n    print(f\"‚ùå S-DoT data directory not found at {SDOT_PATH}\")\n    if paths['environment'] == 'colab':\n        print(\"üí° For Google Colab:\")\n        print(\"   1. Clone repository: !git clone https://github.com/KimJiHan/seoul_climate_analysis.git\")\n        print(\"   2. Or upload data to Google Drive and mount it\")\n    elif paths['environment'] == 'kaggle':\n        print(\"üí° For Kaggle: Upload data as a Kaggle Dataset\")\n    else:\n        print(\"üí° For Local: Please check the data setup or run the data preparation script\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load a sample CSV file to understand the structure\ntry:\n    if 'csv_files' in locals() and csv_files:\n        sample_file = csv_files[0]\n        print(f\"üîç Loading sample file: {os.path.basename(sample_file)}\")\n        \n        # Try different encodings for Korean text\n        encodings_to_try = ['euc-kr', 'cp949', 'utf-8', 'utf-8-sig']\n        df_sample = None\n        \n        for encoding in encodings_to_try:\n            try:\n                print(f\"  Trying encoding: {encoding}\")\n                df_sample = pd.read_csv(sample_file, encoding=encoding, nrows=1000)\n                print(f\"  ‚úÖ Successfully loaded with encoding: {encoding}\")\n                break\n            except UnicodeDecodeError as e:\n                print(f\"  ‚ùå Failed with {encoding}: {str(e)[:50]}...\")\n                continue\n            except Exception as e:\n                print(f\"  ‚ùå Error with {encoding}: {str(e)[:50]}...\")\n                continue\n        \n        if df_sample is not None:\n            print(f\"\\nüìä Sample Data Shape: {df_sample.shape}\")\n            print(f\"üìã Column count: {len(df_sample.columns)}\")\n            print(f\"üìù First few columns: {df_sample.columns.tolist()[:8]}\")\n            \n            # Show basic info about the data\n            print(f\"üìÖ Date range: {df_sample.iloc[0, 2] if len(df_sample.columns) > 2 else 'N/A'}\")\n            print(f\"üìç Sensor count in sample: {df_sample.iloc[:, 1].nunique() if len(df_sample.columns) > 1 else 'N/A'}\")\n            \n        else:\n            raise Exception(\"Could not decode file with any encoding\")\n        \nexcept Exception as e:\n    print(f\"‚ùå Error loading S-DoT data: {e}\")\n    print(\"üìù Creating simulated data for demonstration purposes...\")\n    \n    # Create simulated data for demonstration\n    np.random.seed(42)\n    dates = pd.date_range('2025-04-28', periods=1000, freq='10min')\n    \n    # Create realistic Seoul weather data\n    df_sample = pd.DataFrame({\n        'sensor_id': np.random.choice(['SDOT001', 'SDOT002', 'SDOT003'], 1000),\n        'serial_number': np.random.choice(['OC3CL200011', 'OC3CL200027', 'OC3CL200019'], 1000),\n        'measurement_time': dates,\n        'location_type': np.random.choice(['parks', 'main_street', 'residential'], 1000),\n        'district': np.random.choice(['Gangnam-gu', 'Gangdong-gu', 'Yongsan-gu'], 1000),\n        'temperature': np.random.normal(22, 5, 1000).round(1),\n        'humidity': np.random.uniform(40, 85, 1000).round(0),\n        'pm25': np.random.exponential(25, 1000).round(1),\n        'pm10': np.random.exponential(40, 1000).round(1),\n        'noise': np.random.normal(55, 10, 1000).round(0)\n    })\n    \n    print(\"‚úÖ Simulated dataset created successfully\")\n    print(f\"üìä Shape: {df_sample.shape}\")\n    print(f\"üìã Columns: {df_sample.columns.tolist()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Basic Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"üìã First 5 rows of the data:\")\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"üìä Statistical Summary:\")\n",
    "df_sample.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and missing values\n",
    "print(\"üîç Data Information:\")\n",
    "df_sample.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Simple Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a simple visualization\n# Check for temperature columns (Korean or English)\ntemp_cols = []\n\n# Check for Korean temperature columns\nkorean_temp_cols = [col for col in df_sample.columns if 'Ïò®ÎèÑ' in col and 'ÌùëÍµ¨' not in col]\nif korean_temp_cols:\n    temp_cols = korean_temp_cols\n    print(f\"üå°Ô∏è Found Korean temperature columns: {temp_cols}\")\n\n# Check for English temperature columns\nenglish_temp_cols = [col for col in df_sample.columns if 'temperature' in col.lower() or 'temp' in col.lower()]\nif english_temp_cols and not temp_cols:\n    temp_cols = english_temp_cols\n    print(f\"üå°Ô∏è Found English temperature columns: {temp_cols}\")\n\nif temp_cols:\n    # Use average temperature if available, otherwise use first temperature column\n    if 'Ïò®ÎèÑ ÌèâÍ∑†(‚ÑÉ)' in temp_cols:\n        temp_col = 'Ïò®ÎèÑ ÌèâÍ∑†(‚ÑÉ)'\n    elif 'temperature' in df_sample.columns:\n        temp_col = 'temperature'\n    else:\n        temp_col = temp_cols[0]\n    \n    print(f\"üìä Using temperature column: {temp_col}\")\n    \n    # Check for time column\n    time_cols = [col for col in df_sample.columns if 'ÏãúÍ∞Ñ' in col or 'time' in col.lower() or 'date' in col.lower()]\n    time_col = time_cols[0] if time_cols else None\n    \n    import matplotlib.pyplot as plt\n    import numpy as np\n    \n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Get temperature data and remove NaN values\n    temp_data = pd.to_numeric(df_sample[temp_col], errors='coerce').dropna()\n    \n    if len(temp_data) > 0:\n        # Histogram\n        axes[0].hist(temp_data, bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n        axes[0].set_xlabel('Temperature (¬∞C)')\n        axes[0].set_ylabel('Frequency')\n        axes[0].set_title('Temperature Distribution')\n        axes[0].grid(True, alpha=0.3)\n        \n        # Add statistics\n        mean_temp = temp_data.mean()\n        std_temp = temp_data.std()\n        axes[0].axvline(mean_temp, color='red', linestyle='--', alpha=0.7, label=f'Mean: {mean_temp:.1f}¬∞C')\n        axes[0].legend()\n        \n        # Time series (if time column exists)\n        if time_col:\n            try:\n                # Convert time column to datetime\n                df_sample['datetime_parsed'] = pd.to_datetime(df_sample[time_col], errors='coerce')\n                \n                # Filter valid datetime and temperature data\n                valid_mask = df_sample['datetime_parsed'].notna() & df_sample[temp_col].notna()\n                valid_data = df_sample[valid_mask].copy()\n                \n                if len(valid_data) > 0:\n                    # Sort by time\n                    valid_data = valid_data.sort_values('datetime_parsed')\n                    \n                    # Plot first 100 points or all if less\n                    plot_data = valid_data.head(min(100, len(valid_data)))\n                    axes[1].plot(plot_data['datetime_parsed'], plot_data[temp_col], \n                               alpha=0.7, color='orange', linewidth=1.5, marker='o', markersize=3)\n                    axes[1].set_xlabel('Time')\n                    axes[1].set_ylabel('Temperature (¬∞C)')\n                    axes[1].set_title(f'Temperature Time Series ({len(plot_data)} observations)')\n                    axes[1].grid(True, alpha=0.3)\n                    plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=45)\n                else:\n                    axes[1].text(0.5, 0.5, 'No valid time series data', \n                               transform=axes[1].transAxes, ha='center', va='center')\n            except Exception as e:\n                axes[1].text(0.5, 0.5, f'Time series error:\\n{str(e)[:50]}', \n                           transform=axes[1].transAxes, ha='center', va='center')\n        else:\n            axes[1].text(0.5, 0.5, 'No time column found', \n                       transform=axes[1].transAxes, ha='center', va='center')\n        \n        plt.tight_layout()\n        plt.show()\n        \n        # Print statistics\n        print(f\"\\nüìä Temperature Statistics:\")\n        print(f\"   Mean: {mean_temp:.2f}¬∞C\")\n        print(f\"   Std:  {std_temp:.2f}¬∞C\")\n        print(f\"   Min:  {temp_data.min():.2f}¬∞C\")\n        print(f\"   Max:  {temp_data.max():.2f}¬∞C\")\n        print(f\"   Count: {len(temp_data)} valid measurements\")\n    else:\n        print(\"‚ö†Ô∏è No valid temperature data found for visualization\")\n        print(\"   This might be due to missing values in the sample\")\n        \nelse:\n    print(\"‚ùå Temperature column not found in the data\")\n    print(f\"Available columns: {df_sample.columns.tolist()[:10]}...\")\n    print(\"\\nüí° Tip: Make sure S-DoT data is loaded correctly in Section 4.2\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Git/GitHub Setup\n\n### 6.1 Environment-specific Git Commands\n\n**For Local Environment:**\nRun these commands in your terminal (not in Jupyter):\n\n```bash\n# Configure Git (first time only)\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n\n# Initialize repository\ngit init\n\n# Add files\ngit add .\n\n# Commit changes\ngit commit -m \"Initial commit: Week 1 notebook\"\n\n# Connect to GitHub repository\ngit remote add origin https://github.com/KimJiHan/seoul_climate_analysis.git\n\n# Push to GitHub\ngit push -u origin main\n```\n\n**For Google Colab:**\n```python\n# Clone the repository\n!git clone https://github.com/KimJiHan/seoul_climate_analysis.git\n%cd seoul_climate_analysis/seoul_heatwave_course\n\n# Configure Git in Colab\n!git config --global user.name \"Your Name\"\n!git config --global user.email \"your.email@example.com\"\n\n# Create and switch to your branch\n!git checkout -b week01-yourname\n\n# After completing work\n!git add .\n!git commit -m \"Complete Week 1 assignment\"\n!git push origin week01-yourname\n```\n\n**For Kaggle:**\nKaggle notebooks can be forked and shared directly through the Kaggle platform. You can also download the notebook and commit to GitHub separately."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Assignment\n",
    "\n",
    "### Week 1 Tasks:\n",
    "\n",
    "1. **Environment Setup** (20 points)\n",
    "   - Install all required packages\n",
    "   - Verify Python version\n",
    "   - Configure Jupyter environment\n",
    "\n",
    "2. **Data Exploration** (30 points)\n",
    "   - Load at least 3 S-DoT CSV files\n",
    "   - Identify all column names and data types\n",
    "   - Calculate basic statistics for temperature and humidity\n",
    "\n",
    "3. **Visualization** (30 points)\n",
    "   - Create a histogram for each numerical variable\n",
    "   - Plot a time series for one day of temperature data\n",
    "   - Create a correlation matrix heatmap\n",
    "\n",
    "4. **GitHub Setup** (20 points)\n",
    "   - Fork the course repository\n",
    "   - Create a branch with your name\n",
    "   - Submit your completed notebook via Pull Request\n",
    "\n",
    "### Submission Instructions:\n",
    "1. Complete this notebook with your code\n",
    "2. Save as `Week01_YourName.ipynb`\n",
    "3. Push to your GitHub fork\n",
    "4. Create a Pull Request with title: `Week 1 Assignment - Your Name`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "In this week, we covered:\n",
    "- ‚úÖ Project background and importance\n",
    "- ‚úÖ S-DoT sensor network overview\n",
    "- ‚úÖ Python environment setup\n",
    "- ‚úÖ Basic data loading and exploration\n",
    "- ‚úÖ Simple visualizations\n",
    "- ‚úÖ Git/GitHub workflow\n",
    "\n",
    "### Next Week Preview:\n",
    "**Week 2: Data Collection & Preprocessing**\n",
    "- Loading multiple S-DoT files\n",
    "- Data cleaning techniques\n",
    "- Handling missing values\n",
    "- Data integration strategies\n",
    "\n",
    "### Resources:\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- [Matplotlib Tutorial](https://matplotlib.org/stable/tutorials/index.html)\n",
    "- [Git Handbook](https://guides.github.com/introduction/git-handbook/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**End of Week 1**\n",
    "\n",
    "*Instructor: Sohn Chul*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}