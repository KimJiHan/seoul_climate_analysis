{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Data Collection and Preprocessing\n",
    "## Loading and Cleaning S-DoT Sensor Data\n",
    "\n",
    "**Instructor**: Sohn Chul\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "1. Load and combine multiple S-DoT CSV files efficiently\n",
    "2. Identify and handle missing values appropriately\n",
    "3. Detect and remove outliers in sensor data\n",
    "4. Perform data type conversions and datetime parsing\n",
    "5. Create a clean, analysis-ready dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Datetime handling\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Progress bar for loops\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure visualization\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"âœ… Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding S-DoT Data Structure\n",
    "\n",
    "### 2.1 Data Characteristics\n",
    "\n",
    "S-DoT sensor data has the following characteristics:\n",
    "- **Temporal Resolution**: 10-minute intervals\n",
    "- **File Organization**: Weekly CSV files (April-August 2025)\n",
    "- **Expected Files**: 16 weekly files covering 5 months\n",
    "- **Variables**: Temperature, Humidity, PM2.5, PM10, Noise\n",
    "- **Encoding**: UTF-8 with Korean column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "BASE_PATH = '../..'  # Adjust based on your directory structure\n",
    "SDOT_PATH = os.path.join(BASE_PATH, 's-dot')\n",
    "OUTPUT_PATH = '../data/processed'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"ðŸ“ S-DoT data path: {SDOT_PATH}\")\n",
    "print(f\"ðŸ“ Output path: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Collection\n",
    "\n",
    "### 3.1 List Available CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all CSV files in the S-DoT directory\n",
    "csv_files = glob.glob(os.path.join(SDOT_PATH, '*.csv'))\n",
    "csv_files.sort()  # Sort files chronologically\n",
    "\n",
    "print(f\"ðŸ“Š Found {len(csv_files)} CSV files:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "total_size = 0\n",
    "for i, file in enumerate(csv_files, 1):\n",
    "    file_size = os.path.getsize(file) / (1024 * 1024)  # Convert to MB\n",
    "    total_size += file_size\n",
    "    print(f\"{i:2d}. {os.path.basename(file):40s} {file_size:8.2f} MB\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"Total size: {total_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load Sample File to Understand Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first file to understand structure\n",
    "if csv_files:\n",
    "    sample_file = csv_files[0]\n",
    "    print(f\"Loading sample: {os.path.basename(sample_file)}\")\n",
    "    \n",
    "    # Read with Korean encoding\n",
    "    df_sample = pd.read_csv(sample_file, encoding='utf-8', nrows=5)\n",
    "    \n",
    "    print(\"\\nðŸ“‹ Column names:\")\n",
    "    for i, col in enumerate(df_sample.columns, 1):\n",
    "        print(f\"  {i:2d}. {col}\")\n",
    "    \n",
    "    print(\"\\nðŸ“Š Sample data:\")\n",
    "    display(df_sample.head())\n",
    "    \n",
    "    print(\"\\nðŸ“ˆ Data types:\")\n",
    "    print(df_sample.dtypes)\n",
    "else:\n",
    "    print(\"No CSV files found. Creating sample data for demonstration...\")\n",
    "    \n",
    "    # Create sample data structure\n",
    "    df_sample = pd.DataFrame({\n",
    "        'ì‹œë¦¬ì–¼ë²ˆí˜¸': ['S001'] * 5,\n",
    "        'ì¸¡ì •ì‹œê°„': pd.date_range('2025-04-01', periods=5, freq='10min'),\n",
    "        'ê¸°ì˜¨': np.random.normal(20, 5, 5),\n",
    "        'ìŠµë„': np.random.uniform(40, 80, 5),\n",
    "        'ë¯¸ì„¸ë¨¼ì§€': np.random.exponential(25, 5),\n",
    "        'ì´ˆë¯¸ì„¸ë¨¼ì§€': np.random.exponential(15, 5),\n",
    "        'ì†ŒìŒ': np.random.normal(65, 10, 5)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Define Column Mapping for Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column name mapping (Korean to English)\n",
    "COLUMN_MAPPING = {\n",
    "    'ì‹œë¦¬ì–¼ë²ˆí˜¸': 'serial_number',\n",
    "    'ì¸¡ì •ì‹œê°„': 'datetime',\n",
    "    'ì¸¡ì •ì¼ì‹œ': 'datetime',  # Alternative name\n",
    "    'ê¸°ì˜¨': 'temperature',\n",
    "    'ì˜¨ë„': 'temperature',  # Alternative name\n",
    "    'ìŠµë„': 'humidity',\n",
    "    'ìƒëŒ€ìŠµë„': 'humidity',  # Alternative name\n",
    "    'ë¯¸ì„¸ë¨¼ì§€': 'pm10',\n",
    "    'PM10': 'pm10',\n",
    "    'ì´ˆë¯¸ì„¸ë¨¼ì§€': 'pm25',\n",
    "    'PM2.5': 'pm25',\n",
    "    'ì†ŒìŒ': 'noise',\n",
    "    'ì†ŒìŒë„': 'noise'  # Alternative name\n",
    "}\n",
    "\n",
    "print(\"Column mapping defined:\")\n",
    "for korean, english in list(COLUMN_MAPPING.items())[:6]:\n",
    "    print(f\"  {korean:10s} â†’ {english}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Data Loading\n",
    "\n",
    "### 4.1 Function to Load and Standardize Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_standardize_file(filepath, column_mapping=COLUMN_MAPPING):\n",
    "    \"\"\"\n",
    "    Load a single CSV file and standardize column names.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath : str\n",
    "        Path to CSV file\n",
    "    column_mapping : dict\n",
    "        Dictionary mapping original to standardized column names\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Standardized dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load CSV file\n",
    "        df = pd.read_csv(filepath, encoding='utf-8')\n",
    "        \n",
    "        # Rename columns\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        \n",
    "        # Add source file information\n",
    "        df['source_file'] = os.path.basename(filepath)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test the function\n",
    "if csv_files:\n",
    "    test_df = load_and_standardize_file(csv_files[0])\n",
    "    if test_df is not None:\n",
    "        print(f\"âœ… Successfully loaded file with {len(test_df)} rows\")\n",
    "        print(f\"Columns: {test_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Load All Files with Progress Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_sdot_files(file_list, column_mapping=COLUMN_MAPPING):\n",
    "    \"\"\"\n",
    "    Load all S-DoT CSV files and combine into single dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_list : list\n",
    "        List of file paths\n",
    "    column_mapping : dict\n",
    "        Column name mapping\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Combined dataframe\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "    \n",
    "    print(f\"Loading {len(file_list)} files...\")\n",
    "    \n",
    "    for filepath in tqdm(file_list, desc=\"Loading files\"):\n",
    "        df = load_and_standardize_file(filepath, column_mapping)\n",
    "        if df is not None:\n",
    "            dataframes.append(df)\n",
    "    \n",
    "    if dataframes:\n",
    "        # Combine all dataframes\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "        print(f\"\\nâœ… Successfully loaded {len(dataframes)} files\")\n",
    "        print(f\"Total rows: {len(combined_df):,}\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"âŒ No data loaded\")\n",
    "        return None\n",
    "\n",
    "# Load all data (or subset for testing)\n",
    "# For testing, load only first 3 files\n",
    "files_to_load = csv_files[:3] if len(csv_files) > 3 else csv_files\n",
    "\n",
    "if files_to_load:\n",
    "    df_all = load_all_sdot_files(files_to_load)\n",
    "else:\n",
    "    # Create synthetic data for demonstration\n",
    "    print(\"Creating synthetic data for demonstration...\")\n",
    "    \n",
    "    dates = pd.date_range('2025-04-01', '2025-04-07', freq='10min')\n",
    "    n_sensors = 5\n",
    "    \n",
    "    df_all = pd.DataFrame({\n",
    "        'serial_number': np.repeat([f'S{i:03d}' for i in range(1, n_sensors+1)], len(dates)),\n",
    "        'datetime': np.tile(dates, n_sensors),\n",
    "        'temperature': np.random.normal(22, 5, len(dates) * n_sensors),\n",
    "        'humidity': np.random.uniform(40, 80, len(dates) * n_sensors),\n",
    "        'pm10': np.random.exponential(40, len(dates) * n_sensors),\n",
    "        'pm25': np.random.exponential(25, len(dates) * n_sensors),\n",
    "        'noise': np.random.normal(65, 10, len(dates) * n_sensors),\n",
    "        'source_file': 'synthetic_data.csv'\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing\n",
    "\n",
    "### 5.1 Data Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current data types\n",
    "print(\"Current data types:\")\n",
    "print(df_all.dtypes)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Convert datetime column\n",
    "if 'datetime' in df_all.columns:\n",
    "    df_all['datetime'] = pd.to_datetime(df_all['datetime'])\n",
    "    print(\"âœ… Datetime column converted\")\n",
    "\n",
    "# Ensure numeric columns are float\n",
    "numeric_columns = ['temperature', 'humidity', 'pm10', 'pm25', 'noise']\n",
    "for col in numeric_columns:\n",
    "    if col in df_all.columns:\n",
    "        df_all[col] = pd.to_numeric(df_all[col], errors='coerce')\n",
    "        print(f\"âœ… {col} converted to numeric\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Updated data types:\")\n",
    "print(df_all.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_counts = df_all.isnull().sum()\n",
    "missing_percent = (missing_counts / len(df_all)) * 100\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing_Count': missing_counts,\n",
    "    'Missing_Percent': missing_percent\n",
    "})\n",
    "\n",
    "print(\"Missing Value Summary:\")\n",
    "print(missing_summary[missing_summary['Missing_Count'] > 0])\n",
    "\n",
    "# Visualize missing patterns\n",
    "if missing_counts.sum() > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    missing_summary[missing_summary['Missing_Count'] > 0]['Missing_Percent'].plot(kind='bar')\n",
    "    plt.title('Missing Values by Column')\n",
    "    plt.xlabel('Column')\n",
    "    plt.ylabel('Missing Percentage (%)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nâœ… No missing values found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, method='interpolate'):\n",
    "    \"\"\"\n",
    "    Handle missing values in the dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    method : str\n",
    "        Method to handle missing values:\n",
    "        - 'drop': Remove rows with missing values\n",
    "        - 'forward_fill': Forward fill\n",
    "        - 'interpolate': Linear interpolation\n",
    "        - 'mean': Fill with mean value\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Dataframe with handled missing values\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    numeric_columns = df_processed.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if method == 'drop':\n",
    "        df_processed = df_processed.dropna()\n",
    "    \n",
    "    elif method == 'forward_fill':\n",
    "        df_processed[numeric_columns] = df_processed[numeric_columns].fillna(method='ffill')\n",
    "    \n",
    "    elif method == 'interpolate':\n",
    "        # Group by sensor and interpolate\n",
    "        if 'serial_number' in df_processed.columns:\n",
    "            for col in numeric_columns:\n",
    "                df_processed[col] = df_processed.groupby('serial_number')[col].transform(\n",
    "                    lambda x: x.interpolate(method='linear', limit_direction='both')\n",
    "                )\n",
    "        else:\n",
    "            df_processed[numeric_columns] = df_processed[numeric_columns].interpolate(method='linear')\n",
    "    \n",
    "    elif method == 'mean':\n",
    "        for col in numeric_columns:\n",
    "            df_processed[col].fillna(df_processed[col].mean(), inplace=True)\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Apply missing value handling\n",
    "df_clean = handle_missing_values(df_all, method='interpolate')\n",
    "\n",
    "# Check results\n",
    "print(\"Missing values after handling:\")\n",
    "print(df_clean.isnull().sum())\n",
    "print(f\"\\nâœ… Missing values handled using interpolation\")\n",
    "print(f\"Original shape: {df_all.shape}\")\n",
    "print(f\"Clean shape: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Outlier Detection and Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(df, columns, threshold=1.5):\n",
    "    \"\"\"\n",
    "    Detect outliers using IQR method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    columns : list\n",
    "        Columns to check for outliers\n",
    "    threshold : float\n",
    "        IQR multiplier for outlier detection\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Boolean dataframe indicating outliers\n",
    "    \"\"\"\n",
    "    outliers = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            lower_bound = Q1 - threshold * IQR\n",
    "            upper_bound = Q3 + threshold * IQR\n",
    "            \n",
    "            outliers[col] = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "            \n",
    "            n_outliers = outliers[col].sum()\n",
    "            percent = (n_outliers / len(df)) * 100\n",
    "            \n",
    "            print(f\"{col:15s}: {n_outliers:6d} outliers ({percent:.2f}%)\")\n",
    "            print(f\"  Range: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# Detect outliers\n",
    "print(\"Outlier Detection Results:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "numeric_cols = ['temperature', 'humidity', 'pm10', 'pm25', 'noise']\n",
    "outliers = detect_outliers_iqr(df_clean, numeric_cols, threshold=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    if col in df_clean.columns and i < len(axes):\n",
    "        axes[i].boxplot(df_clean[col].dropna())\n",
    "        axes[i].set_title(f'{col.capitalize()} Distribution')\n",
    "        axes[i].set_ylabel('Value')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Hide extra subplot\n",
    "if len(numeric_cols) < len(axes):\n",
    "    axes[-1].set_visible(False)\n",
    "\n",
    "plt.suptitle('Outlier Detection using Boxplots', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Apply Domain-Specific Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_domain_constraints(df):\n",
    "    \"\"\"\n",
    "    Apply domain-specific constraints to ensure data quality.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Cleaned dataframe\n",
    "    \"\"\"\n",
    "    df_constrained = df.copy()\n",
    "    \n",
    "    # Temperature: -30Â°C to 50Â°C (reasonable range for Seoul)\n",
    "    if 'temperature' in df_constrained.columns:\n",
    "        mask = (df_constrained['temperature'] >= -30) & (df_constrained['temperature'] <= 50)\n",
    "        n_removed = (~mask).sum()\n",
    "        df_constrained = df_constrained[mask]\n",
    "        print(f\"Temperature: Removed {n_removed} invalid values\")\n",
    "    \n",
    "    # Humidity: 0% to 100%\n",
    "    if 'humidity' in df_constrained.columns:\n",
    "        mask = (df_constrained['humidity'] >= 0) & (df_constrained['humidity'] <= 100)\n",
    "        n_removed = (~mask).sum()\n",
    "        df_constrained = df_constrained[mask]\n",
    "        print(f\"Humidity: Removed {n_removed} invalid values\")\n",
    "    \n",
    "    # PM2.5 and PM10: >= 0\n",
    "    for col in ['pm25', 'pm10']:\n",
    "        if col in df_constrained.columns:\n",
    "            mask = df_constrained[col] >= 0\n",
    "            n_removed = (~mask).sum()\n",
    "            df_constrained = df_constrained[mask]\n",
    "            print(f\"{col.upper()}: Removed {n_removed} negative values\")\n",
    "    \n",
    "    # Noise: 0 to 150 dB\n",
    "    if 'noise' in df_constrained.columns:\n",
    "        mask = (df_constrained['noise'] >= 0) & (df_constrained['noise'] <= 150)\n",
    "        n_removed = (~mask).sum()\n",
    "        df_constrained = df_constrained[mask]\n",
    "        print(f\"Noise: Removed {n_removed} invalid values\")\n",
    "    \n",
    "    return df_constrained\n",
    "\n",
    "# Apply constraints\n",
    "print(\"Applying domain-specific constraints:\")\n",
    "print(\"=\"*50)\n",
    "df_final = apply_domain_constraints(df_clean)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Final dataset shape: {df_final.shape}\")\n",
    "print(f\"Data loss: {(1 - len(df_final)/len(df_clean))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quality_report(df):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive data quality report.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Cleaned dataframe\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Quality metrics\n",
    "    \"\"\"\n",
    "    report = {}\n",
    "    \n",
    "    # Basic information\n",
    "    report['total_records'] = len(df)\n",
    "    report['date_range'] = f\"{df['datetime'].min()} to {df['datetime'].max()}\" if 'datetime' in df.columns else 'N/A'\n",
    "    \n",
    "    # Sensor information\n",
    "    if 'serial_number' in df.columns:\n",
    "        report['unique_sensors'] = df['serial_number'].nunique()\n",
    "    \n",
    "    # Data completeness\n",
    "    report['completeness'] = (1 - df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100\n",
    "    \n",
    "    # Statistical summary\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    report['statistics'] = df[numeric_cols].describe().to_dict()\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate report\n",
    "quality_report = generate_quality_report(df_final)\n",
    "\n",
    "print(\"ðŸ“Š Data Quality Report\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total Records: {quality_report['total_records']:,}\")\n",
    "print(f\"Date Range: {quality_report['date_range']}\")\n",
    "if 'unique_sensors' in quality_report:\n",
    "    print(f\"Unique Sensors: {quality_report['unique_sensors']}\")\n",
    "print(f\"Data Completeness: {quality_report['completeness']:.2f}%\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ Statistical Summary:\")\n",
    "stats_df = pd.DataFrame(quality_report['statistics'])\n",
    "print(stats_df.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data\n",
    "output_file = os.path.join(OUTPUT_PATH, 'sdot_cleaned_data.csv')\n",
    "\n",
    "try:\n",
    "    df_final.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    file_size = os.path.getsize(output_file) / (1024 * 1024)  # MB\n",
    "    print(f\"âœ… Data saved successfully!\")\n",
    "    print(f\"ðŸ“ File: {output_file}\")\n",
    "    print(f\"ðŸ“Š Size: {file_size:.2f} MB\")\n",
    "    print(f\"ðŸ“ˆ Records: {len(df_final):,}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error saving file: {e}\")\n",
    "\n",
    "# Also save a smaller sample for quick testing\n",
    "sample_file = os.path.join(OUTPUT_PATH, 'sdot_sample.csv')\n",
    "df_final.head(10000).to_csv(sample_file, index=False, encoding='utf-8')\n",
    "print(f\"\\nðŸ“ Sample file saved: {sample_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Assignment\n",
    "\n",
    "### Week 2 Tasks:\n",
    "\n",
    "1. **Data Loading** (25 points)\n",
    "   - Load all 16 S-DoT CSV files for April-August 2025\n",
    "   - Combine into a single dataframe\n",
    "   - Document any loading issues encountered\n",
    "\n",
    "2. **Data Cleaning** (30 points)\n",
    "   - Handle missing values using appropriate methods\n",
    "   - Identify and document outliers\n",
    "   - Apply domain-specific constraints\n",
    "\n",
    "3. **Data Analysis** (25 points)\n",
    "   - Calculate the percentage of missing data per column\n",
    "   - Create visualizations showing data distribution\n",
    "   - Generate a data quality report\n",
    "\n",
    "4. **Documentation** (20 points)\n",
    "   - Document your preprocessing decisions\n",
    "   - Explain why you chose specific methods\n",
    "   - Create a summary of data characteristics\n",
    "\n",
    "### Bonus Challenge (10 extra points):\n",
    "- Implement a function to detect sensor malfunctions (e.g., constant values, sudden jumps)\n",
    "- Create a visualization showing data availability over time for each sensor\n",
    "\n",
    "### Submission:\n",
    "- Complete notebook saved as `Week02_YourName.ipynb`\n",
    "- Include your cleaned dataset (or a sample if too large)\n",
    "- Submit via GitHub Pull Request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "In this week, we covered:\n",
    "- âœ… Loading multiple S-DoT CSV files\n",
    "- âœ… Standardizing column names\n",
    "- âœ… Handling missing values\n",
    "- âœ… Detecting and removing outliers\n",
    "- âœ… Applying domain constraints\n",
    "- âœ… Generating data quality reports\n",
    "- âœ… Saving clean datasets\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **Data Quality is Critical**: Clean data is essential for accurate analysis\n",
    "2. **Domain Knowledge Matters**: Understanding sensor limitations helps in preprocessing\n",
    "3. **Documentation is Important**: Track all preprocessing decisions\n",
    "4. **Efficiency Considerations**: Use batch processing for large datasets\n",
    "\n",
    "### Next Week Preview:\n",
    "**Week 3: Heat Index Calculation**\n",
    "- Implement NOAA Heat Index formula\n",
    "- Temperature unit conversions\n",
    "- Calculate daily maximum heat index\n",
    "- Identify heat wave events\n",
    "\n",
    "---\n",
    "**End of Week 2**\n",
    "\n",
    "*Instructor: Sohn Chul*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}