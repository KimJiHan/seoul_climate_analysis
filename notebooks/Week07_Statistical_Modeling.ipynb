{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7: Statistical Modeling and Prediction\n",
    "## Regression Analysis and Heat Index Forecasting\n",
    "\n",
    "**Instructor**: Sohn Chul\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "1. Build regression models for KMA heat index prediction\n",
    "2. Perform correlation and causation analysis\n",
    "3. Implement time series forecasting models\n",
    "4. Validate model performance and accuracy\n",
    "5. Create ensemble models for improved predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistical modeling\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Time series\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Other utilities\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate and Prepare Data with KMA Heat Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive dataset for modeling\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create date range\n",
    "dates = pd.date_range('2025-04-01', '2025-08-31', freq='H')\n",
    "n = len(dates)\n",
    "\n",
    "# Generate correlated weather variables\n",
    "hours = dates.hour\n",
    "days = (dates - dates[0]).days\n",
    "\n",
    "# Temperature with patterns\n",
    "temp_base = 20 + (days / 30) * 3  # Seasonal trend\n",
    "temp_daily = 7 * np.sin((hours - 6) * np.pi / 12)  # Daily cycle\n",
    "temperature = temp_base + temp_daily + np.random.normal(0, 2, n)\n",
    "\n",
    "# Humidity (inversely correlated with temperature)\n",
    "humidity = 75 - temperature * 0.8 + np.random.normal(0, 8, n)\n",
    "humidity = np.clip(humidity, 30, 95)\n",
    "\n",
    "# Wind speed (affects perceived temperature)\n",
    "wind_speed = np.abs(np.random.normal(2, 1.5, n))\n",
    "\n",
    "# Solar radiation (correlated with hour of day)\n",
    "solar_radiation = np.maximum(0, 500 * np.sin((hours - 6) * np.pi / 12) + np.random.normal(0, 50, n))\n",
    "\n",
    "# Air pressure\n",
    "pressure = 1013 + np.random.normal(0, 10, n)\n",
    "\n",
    "# KMA Heat Index Calculation\n",
    "def calculate_wet_bulb_temperature(Ta, RH):\n",
    "    \"\"\"Calculate wet-bulb temperature using Stull's formula.\"\"\"\n",
    "    Tw = (Ta * np.arctan(0.151977 * (RH + 8.313659)**0.5) + \n",
    "          np.arctan(Ta + RH) - \n",
    "          np.arctan(RH - 1.67633) + \n",
    "          0.00391838 * RH**1.5 * np.arctan(0.023101 * RH) - \n",
    "          4.686035)\n",
    "    return Tw\n",
    "\n",
    "def calculate_heat_index_kma(Ta, RH):\n",
    "    \"\"\"Calculate heat index using KMA formula.\"\"\"\n",
    "    Tw = calculate_wet_bulb_temperature(Ta, RH)\n",
    "    HI = (-0.2442 + 0.55399 * Tw + 0.45535 * Ta - \n",
    "          0.0022 * Tw**2 + 0.00278 * Tw * Ta + 3.0)\n",
    "    return HI\n",
    "\n",
    "# Calculate KMA heat index\n",
    "heat_index = calculate_heat_index_kma(temperature, humidity)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'datetime': dates,\n",
    "    'temperature': temperature,\n",
    "    'humidity': humidity,\n",
    "    'wind_speed': wind_speed,\n",
    "    'solar_radiation': solar_radiation,\n",
    "    'pressure': pressure,\n",
    "    'heat_index': heat_index\n",
    "})\n",
    "\n",
    "# Add time-based features\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Add lag features for time series\n",
    "df['temp_lag_1'] = df['temperature'].shift(1)\n",
    "df['temp_lag_24'] = df['temperature'].shift(24)\n",
    "df['hi_lag_1'] = df['heat_index'].shift(1)\n",
    "df['hi_lag_24'] = df['heat_index'].shift(24)\n",
    "\n",
    "# Remove NaN values from lag features\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"‚úÖ Dataset created with {len(df)} records\")\n",
    "print(\"\\nüìä Data Summary:\")\n",
    "print(df[['temperature', 'humidity', 'heat_index']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "correlation_vars = ['temperature', 'humidity', 'wind_speed', 'solar_radiation', 'pressure', 'heat_index']\n",
    "corr_matrix = df[correlation_vars].corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "            cmap='RdBu_r', center=0, vmin=-1, vmax=1,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix of Weather Variables and KMA Heat Index', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print strong correlations with heat index\n",
    "hi_corr = corr_matrix['heat_index'].sort_values(ascending=False)\n",
    "print(\"\\nüîç Correlations with KMA Heat Index:\")\n",
    "print(\"=\"*40)\n",
    "for var, corr in hi_corr.items():\n",
    "    if var != 'heat_index':\n",
    "        print(f\"{var:15s}: {corr:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Linear Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "feature_cols = ['temperature', 'humidity', 'wind_speed', 'solar_radiation', \n",
    "                'pressure', 'hour', 'month', 'is_weekend']\n",
    "X = df[feature_cols]\n",
    "y = df['heat_index']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train multiple linear models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=0.1)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train_scaled)\n",
    "    y_pred_test = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    \n",
    "    results[name] = {\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae,\n",
    "        'predictions': y_pred_test\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "print(\"üìä Linear Model Performance:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Model':<20} {'Train R¬≤':>10} {'Test R¬≤':>10} {'Test RMSE':>10} {'Test MAE':>10}\")\n",
    "print(\"-\"*70)\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name:<20} {metrics['train_r2']:>10.4f} {metrics['test_r2']:>10.4f} \"\n",
    "          f\"{metrics['test_rmse']:>10.4f} {metrics['test_mae']:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coefficients from Linear Regression\n",
    "lr_model = models['Linear Regression']\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Coefficient': lr_model.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Coefficient plot\n",
    "colors = ['green' if c > 0 else 'red' for c in coefficients['Coefficient']]\n",
    "axes[0].barh(coefficients['Feature'], coefficients['Coefficient'], color=colors, alpha=0.7)\n",
    "axes[0].set_xlabel('Coefficient Value')\n",
    "axes[0].set_title('Linear Regression Feature Coefficients', fontweight='bold')\n",
    "axes[0].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature contribution to R¬≤\n",
    "# Calculate individual R¬≤ contributions\n",
    "r2_contributions = []\n",
    "for feature in feature_cols:\n",
    "    X_single = X_train_scaled[:, feature_cols.index(feature)].reshape(-1, 1)\n",
    "    lr_single = LinearRegression()\n",
    "    lr_single.fit(X_single, y_train)\n",
    "    r2_single = lr_single.score(X_single, y_train)\n",
    "    r2_contributions.append(r2_single)\n",
    "\n",
    "contribution_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'R¬≤ Contribution': r2_contributions\n",
    "}).sort_values('R¬≤ Contribution', ascending=False)\n",
    "\n",
    "axes[1].bar(contribution_df['Feature'], contribution_df['R¬≤ Contribution'], \n",
    "           color='skyblue', alpha=0.7)\n",
    "axes[1].set_xlabel('Feature')\n",
    "axes[1].set_ylabel('Individual R¬≤ Score')\n",
    "axes[1].set_title('Individual Feature R¬≤ Contributions', fontweight='bold')\n",
    "axes[1].set_xticklabels(contribution_df['Feature'], rotation=45, ha='right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Feature Importance Analysis for KMA Heat Index Prediction', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Feature Coefficients:\")\n",
    "print(coefficients.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create polynomial features (degree 2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_test_poly = poly.transform(X_test_scaled)\n",
    "\n",
    "# Train polynomial regression with Ridge regularization\n",
    "poly_model = Ridge(alpha=1.0)\n",
    "poly_model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_poly_train = poly_model.predict(X_train_poly)\n",
    "y_pred_poly_test = poly_model.predict(X_test_poly)\n",
    "\n",
    "# Calculate metrics\n",
    "poly_train_r2 = r2_score(y_train, y_pred_poly_train)\n",
    "poly_test_r2 = r2_score(y_test, y_pred_poly_test)\n",
    "poly_test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_poly_test))\n",
    "poly_test_mae = mean_absolute_error(y_test, y_pred_poly_test)\n",
    "\n",
    "print(\"üìä Polynomial Regression Performance:\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Train R¬≤: {poly_train_r2:.4f}\")\n",
    "print(f\"Test R¬≤: {poly_test_r2:.4f}\")\n",
    "print(f\"Test RMSE: {poly_test_rmse:.4f}\")\n",
    "print(f\"Test MAE: {poly_test_mae:.4f}\")\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Linear vs Actual\n",
    "axes[0].scatter(y_test, results['Linear Regression']['predictions'], \n",
    "               alpha=0.5, s=10, label='Predictions')\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "            'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual KMA Heat Index (¬∞C)')\n",
    "axes[0].set_ylabel('Predicted Heat Index (¬∞C)')\n",
    "axes[0].set_title('Linear Regression Predictions')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Polynomial vs Actual\n",
    "axes[1].scatter(y_test, y_pred_poly_test, alpha=0.5, s=10, label='Predictions')\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "            'r--', lw=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Actual KMA Heat Index (¬∞C)')\n",
    "axes[1].set_ylabel('Predicted Heat Index (¬∞C)')\n",
    "axes[1].set_title('Polynomial Regression Predictions')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Regression Model Predictions vs Actual KMA Heat Index', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ensemble models\n",
    "ensemble_models = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "}\n",
    "\n",
    "ensemble_results = {}\n",
    "\n",
    "for name, model in ensemble_models.items():\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    \n",
    "    ensemble_results[name] = {\n",
    "        'model': model,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae,\n",
    "        'predictions': y_pred_test\n",
    "    }\n",
    "\n",
    "# Display ensemble results\n",
    "print(\"üìä Ensemble Model Performance:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<20} {'Train R¬≤':>10} {'Test R¬≤':>10} {'Test RMSE':>10} {'Test MAE':>10}\")\n",
    "print(\"-\"*60)\n",
    "for name, metrics in ensemble_results.items():\n",
    "    print(f\"{name:<20} {metrics['train_r2']:>10.4f} {metrics['test_r2']:>10.4f} \"\n",
    "          f\"{metrics['test_rmse']:>10.4f} {metrics['test_mae']:>10.4f}\")\n",
    "\n",
    "# Feature importance from Random Forest\n",
    "rf_model = ensemble_results['Random Forest']['model']\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'], color='teal', alpha=0.7)\n",
    "plt.xlabel('Importance Score')\n",
    "plt.title('Random Forest Feature Importance for KMA Heat Index', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Random Forest Feature Importance:\")\n",
    "print(feature_importance.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Time Series Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare time series data\n",
    "ts_data = df.set_index('datetime')['heat_index'].resample('D').mean()\n",
    "\n",
    "# Split into train and test\n",
    "train_size = int(len(ts_data) * 0.8)\n",
    "ts_train = ts_data[:train_size]\n",
    "ts_test = ts_data[train_size:]\n",
    "\n",
    "# ARIMA Model\n",
    "arima_model = ARIMA(ts_train, order=(2, 1, 2))\n",
    "arima_fit = arima_model.fit()\n",
    "\n",
    "# Make predictions\n",
    "arima_forecast = arima_fit.forecast(steps=len(ts_test))\n",
    "\n",
    "# Exponential Smoothing\n",
    "exp_model = ExponentialSmoothing(ts_train, seasonal='add', seasonal_periods=7)\n",
    "exp_fit = exp_model.fit()\n",
    "exp_forecast = exp_fit.forecast(steps=len(ts_test))\n",
    "\n",
    "# Calculate metrics\n",
    "arima_rmse = np.sqrt(mean_squared_error(ts_test, arima_forecast))\n",
    "arima_mae = mean_absolute_error(ts_test, arima_forecast)\n",
    "exp_rmse = np.sqrt(mean_squared_error(ts_test, exp_forecast))\n",
    "exp_mae = mean_absolute_error(ts_test, exp_forecast)\n",
    "\n",
    "print(\"üìä Time Series Model Performance:\")\n",
    "print(\"=\"*40)\n",
    "print(f\"ARIMA(2,1,2):\")\n",
    "print(f\"  RMSE: {arima_rmse:.4f}\")\n",
    "print(f\"  MAE: {arima_mae:.4f}\")\n",
    "print(f\"\\nExponential Smoothing:\")\n",
    "print(f\"  RMSE: {exp_rmse:.4f}\")\n",
    "print(f\"  MAE: {exp_mae:.4f}\")\n",
    "\n",
    "# Visualize forecasts\n",
    "fig = go.Figure()\n",
    "\n",
    "# Actual data\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=ts_train.index, y=ts_train.values,\n",
    "    mode='lines', name='Training Data',\n",
    "    line=dict(color='blue', width=2)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=ts_test.index, y=ts_test.values,\n",
    "    mode='lines', name='Actual Test Data',\n",
    "    line=dict(color='black', width=2)\n",
    "))\n",
    "\n",
    "# ARIMA forecast\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=ts_test.index, y=arima_forecast,\n",
    "    mode='lines', name='ARIMA Forecast',\n",
    "    line=dict(color='red', width=2, dash='dash')\n",
    "))\n",
    "\n",
    "# Exponential Smoothing forecast\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=ts_test.index, y=exp_forecast,\n",
    "    mode='lines', name='Exp. Smoothing Forecast',\n",
    "    line=dict(color='green', width=2, dash='dot')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Time Series Forecasting of Daily Mean KMA Heat Index',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Heat Index (¬∞C)',\n",
    "    height=500,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Comparison and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all model results\n",
    "all_models = {}\n",
    "\n",
    "# Add linear models\n",
    "for name, metrics in results.items():\n",
    "    all_models[name] = {\n",
    "        'R¬≤ Score': metrics['test_r2'],\n",
    "        'RMSE': metrics['test_rmse'],\n",
    "        'MAE': metrics['test_mae']\n",
    "    }\n",
    "\n",
    "# Add polynomial model\n",
    "all_models['Polynomial Regression'] = {\n",
    "    'R¬≤ Score': poly_test_r2,\n",
    "    'RMSE': poly_test_rmse,\n",
    "    'MAE': poly_test_mae\n",
    "}\n",
    "\n",
    "# Add ensemble models\n",
    "for name, metrics in ensemble_results.items():\n",
    "    all_models[name] = {\n",
    "        'R¬≤ Score': metrics['test_r2'],\n",
    "        'RMSE': metrics['test_rmse'],\n",
    "        'MAE': metrics['test_mae']\n",
    "    }\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(all_models).T\n",
    "comparison_df = comparison_df.sort_values('R¬≤ Score', ascending=False)\n",
    "\n",
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# R¬≤ Score comparison\n",
    "axes[0].barh(comparison_df.index, comparison_df['R¬≤ Score'], color='skyblue', alpha=0.7)\n",
    "axes[0].set_xlabel('R¬≤ Score')\n",
    "axes[0].set_title('Model R¬≤ Score Comparison', fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE comparison\n",
    "axes[1].barh(comparison_df.index, comparison_df['RMSE'], color='coral', alpha=0.7)\n",
    "axes[1].set_xlabel('RMSE')\n",
    "axes[1].set_title('Model RMSE Comparison', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE comparison\n",
    "axes[2].barh(comparison_df.index, comparison_df['MAE'], color='lightgreen', alpha=0.7)\n",
    "axes[2].set_xlabel('MAE')\n",
    "axes[2].set_title('Model MAE Comparison', fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Statistical Model Performance Comparison for KMA Heat Index Prediction', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Model Performance Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Identify best model\n",
    "best_model = comparison_df['R¬≤ Score'].idxmax()\n",
    "print(f\"\\nüèÜ Best Model: {best_model}\")\n",
    "print(f\"   R¬≤ Score: {comparison_df.loc[best_model, 'R¬≤ Score']:.4f}\")\n",
    "print(f\"   RMSE: {comparison_df.loc[best_model, 'RMSE']:.4f}\")\n",
    "print(f\"   MAE: {comparison_df.loc[best_model, 'MAE']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best performing model for residual analysis\n",
    "if 'Random Forest' in ensemble_results:\n",
    "    best_predictions = ensemble_results['Random Forest']['predictions']\n",
    "    model_name = 'Random Forest'\n",
    "else:\n",
    "    best_predictions = y_pred_poly_test\n",
    "    model_name = 'Polynomial Regression'\n",
    "\n",
    "residuals = y_test - best_predictions\n",
    "\n",
    "# Residual plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Residuals vs Fitted\n",
    "axes[0, 0].scatter(best_predictions, residuals, alpha=0.5, s=10)\n",
    "axes[0, 0].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "axes[0, 0].set_xlabel('Fitted Values')\n",
    "axes[0, 0].set_ylabel('Residuals')\n",
    "axes[0, 0].set_title('Residuals vs Fitted Values')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[0, 1])\n",
    "axes[0, 1].set_title('Normal Q-Q Plot')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram of residuals\n",
    "axes[1, 0].hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Residuals')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Distribution of Residuals')\n",
    "axes[1, 0].axvline(x=0, color='red', linestyle='--', linewidth=1)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals over time\n",
    "axes[1, 1].plot(residuals.values, alpha=0.7)\n",
    "axes[1, 1].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "axes[1, 1].set_xlabel('Index')\n",
    "axes[1, 1].set_ylabel('Residuals')\n",
    "axes[1, 1].set_title('Residuals Over Time')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Residual Analysis for {model_name} Model', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical tests\n",
    "# Shapiro-Wilk test for normality\n",
    "shapiro_stat, shapiro_p = stats.shapiro(residuals[:1000])  # Use subset for test\n",
    "print(f\"\\nüìä Residual Analysis:\")\n",
    "print(f\"Mean of residuals: {residuals.mean():.4f}\")\n",
    "print(f\"Std of residuals: {residuals.std():.4f}\")\n",
    "print(f\"\\nShapiro-Wilk test for normality:\")\n",
    "print(f\"  Statistic: {shapiro_stat:.4f}\")\n",
    "print(f\"  p-value: {shapiro_p:.4f}\")\n",
    "if shapiro_p > 0.05:\n",
    "    print(\"  Result: Residuals appear to be normally distributed\")\n",
    "else:\n",
    "    print(\"  Result: Residuals do not appear to be normally distributed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Model and Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "import joblib\n",
    "\n",
    "if 'Random Forest' in ensemble_results:\n",
    "    best_model_obj = ensemble_results['Random Forest']['model']\n",
    "    model_filename = '../models/kma_heat_index_rf_model.pkl'\n",
    "else:\n",
    "    best_model_obj = poly_model\n",
    "    model_filename = '../models/kma_heat_index_poly_model.pkl'\n",
    "\n",
    "# Save model and scaler\n",
    "joblib.dump(best_model_obj, model_filename)\n",
    "joblib.dump(scaler, '../models/kma_heat_index_scaler.pkl')\n",
    "print(f\"‚úÖ Model saved to {model_filename}\")\n",
    "\n",
    "# Generate summary report\n",
    "report = f\"\"\"\n",
    "STATISTICAL MODELING REPORT - KMA HEAT INDEX PREDICTION\n",
    "========================================================\n",
    "\n",
    "DATA SUMMARY:\n",
    "- Total samples: {len(df)}\n",
    "- Training samples: {len(X_train)}\n",
    "- Testing samples: {len(X_test)}\n",
    "- Features used: {', '.join(feature_cols)}\n",
    "\n",
    "MODEL PERFORMANCE:\n",
    "{comparison_df.round(4).to_string()}\n",
    "\n",
    "BEST MODEL: {best_model}\n",
    "- R¬≤ Score: {comparison_df.loc[best_model, 'R¬≤ Score']:.4f}\n",
    "- RMSE: {comparison_df.loc[best_model, 'RMSE']:.4f} ¬∞C\n",
    "- MAE: {comparison_df.loc[best_model, 'MAE']:.4f} ¬∞C\n",
    "\n",
    "KEY FINDINGS:\n",
    "1. Temperature is the strongest predictor of KMA heat index\n",
    "2. Humidity has significant negative correlation with heat index\n",
    "3. Time-based features (hour, month) improve prediction accuracy\n",
    "4. Ensemble models outperform linear models\n",
    "5. Random Forest provides best balance of accuracy and interpretability\n",
    "\n",
    "TIME SERIES FORECASTING:\n",
    "- ARIMA(2,1,2) RMSE: {arima_rmse:.4f} ¬∞C\n",
    "- Exponential Smoothing RMSE: {exp_rmse:.4f} ¬∞C\n",
    "\n",
    "RECOMMENDATIONS:\n",
    "1. Use Random Forest for real-time heat index prediction\n",
    "2. Consider ensemble methods for operational forecasting\n",
    "3. Update models regularly with new data\n",
    "4. Monitor model performance for drift\n",
    "5. Integrate with S-DoT sensor network for real-time predictions\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Save report\n",
    "with open('../reports/statistical_modeling_report.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "print(\"\\n‚úÖ Report saved to ../reports/statistical_modeling_report.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Assignment\n",
    "\n",
    "### Week 7 Tasks:\n",
    "\n",
    "1. **Regression Analysis** (25 points)\n",
    "   - Build multiple regression models for KMA heat index\n",
    "   - Compare linear, polynomial, and regularized models\n",
    "   - Analyze feature importance and coefficients\n",
    "\n",
    "2. **Ensemble Methods** (25 points)\n",
    "   - Implement Random Forest and Gradient Boosting\n",
    "   - Tune hyperparameters using cross-validation\n",
    "   - Compare ensemble performance with linear models\n",
    "\n",
    "3. **Time Series Forecasting** (25 points)\n",
    "   - Apply ARIMA and exponential smoothing\n",
    "   - Forecast daily heat index values\n",
    "   - Evaluate forecast accuracy\n",
    "\n",
    "4. **Model Validation** (25 points)\n",
    "   - Perform residual analysis\n",
    "   - Conduct cross-validation\n",
    "   - Test for overfitting\n",
    "\n",
    "### Bonus Challenge:\n",
    "- Implement neural network for heat index prediction\n",
    "- Create real-time prediction API\n",
    "- Develop uncertainty quantification for predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this week, we covered:\n",
    "- ‚úÖ Multiple regression techniques for KMA heat index prediction\n",
    "- ‚úÖ Feature importance and correlation analysis\n",
    "- ‚úÖ Ensemble methods (Random Forest, Gradient Boosting)\n",
    "- ‚úÖ Time series forecasting with ARIMA\n",
    "- ‚úÖ Model validation and residual analysis\n",
    "\n",
    "### Next Week Preview:\n",
    "**Week 8: Machine Learning Applications**\n",
    "- Deep learning for heat index prediction\n",
    "- Clustering for pattern discovery\n",
    "- Anomaly detection\n",
    "- Real-time prediction systems\n",
    "\n",
    "### Resources:\n",
    "- [Scikit-learn Documentation](https://scikit-learn.org/)\n",
    "- [Statsmodels Time Series](https://www.statsmodels.org/stable/tsa.html)\n",
    "- [Feature Engineering Guide](https://feature-engine.readthedocs.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**End of Week 7**\n",
    "\n",
    "*Instructor: Sohn Chul*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}