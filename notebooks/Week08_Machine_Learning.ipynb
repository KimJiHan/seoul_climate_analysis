{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8: Machine Learning Applications\n",
    "## Advanced ML Techniques for Heatwave Analysis\n",
    "\n",
    "**Instructor**: Sohn Chul\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "1. Implement deep learning models for KMA heat index prediction\n",
    "2. Apply clustering algorithms to identify heatwave patterns\n",
    "3. Develop anomaly detection systems for extreme events\n",
    "4. Build real-time prediction pipelines\n",
    "5. Create interpretable ML models for policy support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Other utilities\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data with KMA Heat Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create date range\n",
    "dates = pd.date_range('2025-04-01', '2025-08-31', freq='10min')\n",
    "n = len(dates)\n",
    "\n",
    "# Generate weather variables\n",
    "hours = dates.hour + dates.minute/60\n",
    "days = (dates - dates[0]).days\n",
    "\n",
    "# Temperature with complex patterns\n",
    "temp_seasonal = 20 + (days / 30) * 3\n",
    "temp_daily = 7 * np.sin((hours - 6) * np.pi / 12)\n",
    "temp_noise = np.random.normal(0, 2, n)\n",
    "temperature = temp_seasonal + temp_daily + temp_noise\n",
    "\n",
    "# Humidity\n",
    "humidity_base = 70 - temperature * 0.5\n",
    "humidity_daily = 10 * np.sin((hours - 12) * np.pi / 12)\n",
    "humidity = humidity_base + humidity_daily + np.random.normal(0, 5, n)\n",
    "humidity = np.clip(humidity, 30, 95)\n",
    "\n",
    "# Additional features\n",
    "wind_speed = np.abs(np.random.normal(2, 1.5, n))\n",
    "solar_radiation = np.maximum(0, 500 * np.sin((hours - 6) * np.pi / 12) + np.random.normal(0, 50, n))\n",
    "pressure = 1013 + np.random.normal(0, 10, n)\n",
    "pm25 = np.abs(np.random.exponential(25, n))\n",
    "pm10 = np.abs(np.random.exponential(40, n))\n",
    "\n",
    "# KMA Heat Index Calculation\n",
    "def calculate_wet_bulb_temperature(Ta, RH):\n",
    "    \"\"\"Calculate wet-bulb temperature using Stull's formula.\"\"\"\n",
    "    Tw = (Ta * np.arctan(0.151977 * (RH + 8.313659)**0.5) + \n",
    "          np.arctan(Ta + RH) - \n",
    "          np.arctan(RH - 1.67633) + \n",
    "          0.00391838 * RH**1.5 * np.arctan(0.023101 * RH) - \n",
    "          4.686035)\n",
    "    return Tw\n",
    "\n",
    "def calculate_heat_index_kma(Ta, RH):\n",
    "    \"\"\"Calculate heat index using KMA formula.\"\"\"\n",
    "    Tw = calculate_wet_bulb_temperature(Ta, RH)\n",
    "    HI = (-0.2442 + 0.55399 * Tw + 0.45535 * Ta - \n",
    "          0.0022 * Tw**2 + 0.00278 * Tw * Ta + 3.0)\n",
    "    return HI\n",
    "\n",
    "# Calculate KMA heat index\n",
    "heat_index = calculate_heat_index_kma(temperature, humidity)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'datetime': dates,\n",
    "    'temperature': temperature,\n",
    "    'humidity': humidity,\n",
    "    'wind_speed': wind_speed,\n",
    "    'solar_radiation': solar_radiation,\n",
    "    'pressure': pressure,\n",
    "    'pm25': pm25,\n",
    "    'pm10': pm10,\n",
    "    'heat_index': heat_index\n",
    "})\n",
    "\n",
    "# Add time features\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Add heat stress categories (KMA standards)\n",
    "df['heat_stress'] = pd.cut(df['heat_index'], \n",
    "                           bins=[-np.inf, 25, 28, 31, 35, np.inf],\n",
    "                           labels=['Comfortable', 'Caution', 'Extreme Caution', \n",
    "                                  'Danger', 'Extreme Danger'])\n",
    "\n",
    "# Resample to hourly for ML models\n",
    "df_hourly = df.set_index('datetime').resample('H').mean()\n",
    "df_hourly['heat_stress'] = df.set_index('datetime')['heat_stress'].resample('H').agg(lambda x: x.mode()[0] if len(x) > 0 else 'Comfortable')\n",
    "df_hourly = df_hourly.reset_index()\n",
    "\n",
    "print(f\"âœ… Dataset created: {len(df_hourly)} hourly records\")\n",
    "print(\"\\nðŸ“Š Heat Index Statistics (KMA):\")\n",
    "print(df_hourly['heat_index'].describe())\n",
    "print(\"\\nðŸ“Š Heat Stress Distribution:\")\n",
    "print(df.groupby('heat_stress').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep Learning Model for Heat Index Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for neural network\n",
    "feature_cols = ['temperature', 'humidity', 'wind_speed', 'solar_radiation', \n",
    "                'pressure', 'pm25', 'pm10', 'hour', 'month', 'is_weekend']\n",
    "\n",
    "X = df_hourly[feature_cols].values\n",
    "y = df_hourly['heat_index'].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "# Build Neural Network\n",
    "def create_nn_model(input_dim):\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and train model\n",
    "nn_model = create_nn_model(X_train_scaled.shape[1])\n",
    "\n",
    "# Early stopping\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train model\n",
    "history = nn_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "nn_loss, nn_mae = nn_model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "nn_predictions = nn_model.predict(X_test_scaled, verbose=0).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "nn_r2 = r2_score(y_test, nn_predictions)\n",
    "nn_rmse = np.sqrt(mean_squared_error(y_test, nn_predictions))\n",
    "\n",
    "print(\"ðŸ§  Neural Network Performance:\")\n",
    "print(f\"  RÂ² Score: {nn_r2:.4f}\")\n",
    "print(f\"  RMSE: {nn_rmse:.4f}Â°C\")\n",
    "print(f\"  MAE: {nn_mae:.4f}Â°C\")\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history.history['loss'], label='Training Loss')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Model Loss During Training')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].scatter(y_test, nn_predictions, alpha=0.5, s=10)\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual KMA Heat Index (Â°C)')\n",
    "axes[1].set_ylabel('Predicted Heat Index (Â°C)')\n",
    "axes[1].set_title('Neural Network Predictions')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Deep Learning Model for KMA Heat Index Prediction', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LSTM for Time Series Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequences for LSTM\n",
    "def create_sequences(data, seq_length, pred_length=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length - pred_length + 1):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length:i+seq_length+pred_length, -1])  # Predict heat index\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Prepare LSTM data\n",
    "seq_length = 24  # Use 24 hours to predict next hour\n",
    "lstm_features = ['temperature', 'humidity', 'wind_speed', 'heat_index']\n",
    "lstm_data = df_hourly[lstm_features].values\n",
    "\n",
    "# Scale data\n",
    "scaler_lstm = MinMaxScaler()\n",
    "lstm_data_scaled = scaler_lstm.fit_transform(lstm_data)\n",
    "\n",
    "# Create sequences\n",
    "X_seq, y_seq = create_sequences(lstm_data_scaled, seq_length)\n",
    "y_seq = y_seq.reshape(-1, 1)  # Reshape for single output\n",
    "\n",
    "# Split data\n",
    "train_size = int(len(X_seq) * 0.8)\n",
    "X_train_lstm = X_seq[:train_size]\n",
    "X_test_lstm = X_seq[train_size:]\n",
    "y_train_lstm = y_seq[:train_size]\n",
    "y_test_lstm = y_seq[train_size:]\n",
    "\n",
    "# Build LSTM model\n",
    "lstm_model = models.Sequential([\n",
    "    layers.LSTM(50, activation='relu', return_sequences=True, \n",
    "                input_shape=(seq_length, len(lstm_features))),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.LSTM(50, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(25, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train LSTM\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_train_lstm, y_train_lstm,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[callbacks.EarlyStopping(patience=10, restore_best_weights=True)],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate LSTM\n",
    "lstm_predictions = lstm_model.predict(X_test_lstm, verbose=0)\n",
    "\n",
    "# Inverse transform predictions\n",
    "# Create dummy array for inverse transform\n",
    "dummy = np.zeros((len(lstm_predictions), len(lstm_features)))\n",
    "dummy[:, -1] = lstm_predictions.flatten()\n",
    "lstm_predictions_actual = scaler_lstm.inverse_transform(dummy)[:, -1]\n",
    "\n",
    "dummy[:, -1] = y_test_lstm.flatten()\n",
    "y_test_lstm_actual = scaler_lstm.inverse_transform(dummy)[:, -1]\n",
    "\n",
    "# Calculate metrics\n",
    "lstm_r2 = r2_score(y_test_lstm_actual, lstm_predictions_actual)\n",
    "lstm_rmse = np.sqrt(mean_squared_error(y_test_lstm_actual, lstm_predictions_actual))\n",
    "\n",
    "print(\"\\nðŸ”® LSTM Time Series Performance:\")\n",
    "print(f\"  RÂ² Score: {lstm_r2:.4f}\")\n",
    "print(f\"  RMSE: {lstm_rmse:.4f}Â°C\")\n",
    "\n",
    "# Visualize LSTM predictions\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(y_test_lstm_actual[:100], label='Actual', alpha=0.7)\n",
    "plt.plot(lstm_predictions_actual[:100], label='LSTM Predicted', alpha=0.7)\n",
    "plt.xlabel('Time (hours)')\n",
    "plt.ylabel('KMA Heat Index (Â°C)')\n",
    "plt.title('LSTM Time Series Predictions (First 100 hours)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. XGBoost for Heat Index Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for XGBoost\n",
    "# Add lag features\n",
    "df_xgb = df_hourly.copy()\n",
    "for lag in [1, 3, 6, 12, 24]:\n",
    "    df_xgb[f'temp_lag_{lag}'] = df_xgb['temperature'].shift(lag)\n",
    "    df_xgb[f'hi_lag_{lag}'] = df_xgb['heat_index'].shift(lag)\n",
    "\n",
    "df_xgb = df_xgb.dropna()\n",
    "\n",
    "# Features for XGBoost\n",
    "xgb_features = feature_cols + [col for col in df_xgb.columns if 'lag' in col]\n",
    "X_xgb = df_xgb[xgb_features]\n",
    "y_xgb = df_xgb['heat_index']\n",
    "\n",
    "# Split data\n",
    "X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(\n",
    "    X_xgb, y_xgb, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train XGBoost\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_xgb, y_train_xgb)\n",
    "\n",
    "# Predictions\n",
    "xgb_predictions = xgb_model.predict(X_test_xgb)\n",
    "\n",
    "# Calculate metrics\n",
    "xgb_r2 = r2_score(y_test_xgb, xgb_predictions)\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y_test_xgb, xgb_predictions))\n",
    "\n",
    "print(\"ðŸš€ XGBoost Performance:\")\n",
    "print(f\"  RÂ² Score: {xgb_r2:.4f}\")\n",
    "print(f\"  RMSE: {xgb_rmse:.4f}Â°C\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': xgb_features,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(15)\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_importance['feature'], feature_importance['importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('XGBoost Feature Importance for KMA Heat Index')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clustering Analysis for Heatwave Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for clustering\n",
    "clustering_features = ['temperature', 'humidity', 'heat_index', 'solar_radiation', 'hour', 'month']\n",
    "X_cluster = df_hourly[clustering_features].dropna()\n",
    "\n",
    "# Scale features\n",
    "scaler_cluster = StandardScaler()\n",
    "X_cluster_scaled = scaler_cluster.fit_transform(X_cluster)\n",
    "\n",
    "# Apply PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_cluster_scaled)\n",
    "\n",
    "# K-Means clustering\n",
    "n_clusters = 5\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(X_cluster_scaled)\n",
    "\n",
    "# DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=10)\n",
    "dbscan_labels = dbscan.fit_predict(X_cluster_scaled)\n",
    "\n",
    "# Hierarchical clustering\n",
    "hierarchical = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "hierarchical_labels = hierarchical.fit_predict(X_cluster_scaled)\n",
    "\n",
    "# Evaluate clustering\n",
    "kmeans_silhouette = silhouette_score(X_cluster_scaled, kmeans_labels)\n",
    "kmeans_db = davies_bouldin_score(X_cluster_scaled, kmeans_labels)\n",
    "\n",
    "print(\"ðŸ“Š Clustering Performance:\")\n",
    "print(f\"K-Means Silhouette Score: {kmeans_silhouette:.4f}\")\n",
    "print(f\"K-Means Davies-Bouldin Score: {kmeans_db:.4f}\")\n",
    "print(f\"DBSCAN unique clusters: {len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)}\")\n",
    "\n",
    "# Visualize clustering results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# K-Means\n",
    "scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels, \n",
    "                          cmap='viridis', alpha=0.6, s=10)\n",
    "axes[0].set_title('K-Means Clustering')\n",
    "axes[0].set_xlabel('First Principal Component')\n",
    "axes[0].set_ylabel('Second Principal Component')\n",
    "plt.colorbar(scatter1, ax=axes[0])\n",
    "\n",
    "# DBSCAN\n",
    "scatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=dbscan_labels, \n",
    "                          cmap='viridis', alpha=0.6, s=10)\n",
    "axes[1].set_title('DBSCAN Clustering')\n",
    "axes[1].set_xlabel('First Principal Component')\n",
    "axes[1].set_ylabel('Second Principal Component')\n",
    "plt.colorbar(scatter2, ax=axes[1])\n",
    "\n",
    "# Hierarchical\n",
    "scatter3 = axes[2].scatter(X_pca[:, 0], X_pca[:, 1], c=hierarchical_labels, \n",
    "                          cmap='viridis', alpha=0.6, s=10)\n",
    "axes[2].set_title('Hierarchical Clustering')\n",
    "axes[2].set_xlabel('First Principal Component')\n",
    "axes[2].set_ylabel('Second Principal Component')\n",
    "plt.colorbar(scatter3, ax=axes[2])\n",
    "\n",
    "plt.suptitle('Clustering Analysis of Heatwave Patterns', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze cluster characteristics\n",
    "X_cluster['cluster'] = kmeans_labels\n",
    "cluster_summary = X_cluster.groupby('cluster')[clustering_features].mean().round(2)\n",
    "\n",
    "print(\"\\nðŸ“Š K-Means Cluster Characteristics:\")\n",
    "print(cluster_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Anomaly Detection for Extreme Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest for anomaly detection\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "anomaly_labels = iso_forest.fit_predict(X_cluster_scaled)\n",
    "\n",
    "# Mark anomalies\n",
    "X_cluster['is_anomaly'] = anomaly_labels == -1\n",
    "anomalies = X_cluster[X_cluster['is_anomaly']]\n",
    "normal = X_cluster[~X_cluster['is_anomaly']]\n",
    "\n",
    "print(f\"ðŸš¨ Anomaly Detection Results:\")\n",
    "print(f\"  Total anomalies detected: {len(anomalies)} ({len(anomalies)/len(X_cluster)*100:.2f}%)\")\n",
    "print(f\"  Normal observations: {len(normal)}\")\n",
    "\n",
    "# Analyze anomaly characteristics\n",
    "print(\"\\nðŸ“Š Anomaly Characteristics:\")\n",
    "print(\"Anomalies:\")\n",
    "print(anomalies[clustering_features].describe().round(2))\n",
    "print(\"\\nNormal:\")\n",
    "print(normal[clustering_features].describe().round(2))\n",
    "\n",
    "# Visualize anomalies\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Scatter plot in PCA space\n",
    "axes[0].scatter(X_pca[~X_cluster['is_anomaly'].values, 0], \n",
    "               X_pca[~X_cluster['is_anomaly'].values, 1], \n",
    "               c='blue', alpha=0.3, s=10, label='Normal')\n",
    "axes[0].scatter(X_pca[X_cluster['is_anomaly'].values, 0], \n",
    "               X_pca[X_cluster['is_anomaly'].values, 1], \n",
    "               c='red', alpha=0.8, s=20, label='Anomaly')\n",
    "axes[0].set_xlabel('First Principal Component')\n",
    "axes[0].set_ylabel('Second Principal Component')\n",
    "axes[0].set_title('Anomaly Detection in PCA Space')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Heat index distribution\n",
    "axes[1].hist(normal['heat_index'], bins=50, alpha=0.5, label='Normal', color='blue')\n",
    "axes[1].hist(anomalies['heat_index'], bins=20, alpha=0.7, label='Anomaly', color='red')\n",
    "axes[1].set_xlabel('KMA Heat Index (Â°C)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Heat Index Distribution: Normal vs Anomaly')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Anomaly Detection for Extreme Heat Events', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Ensemble and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "model_comparison = {\n",
    "    'Neural Network': {'RÂ²': nn_r2, 'RMSE': nn_rmse},\n",
    "    'LSTM': {'RÂ²': lstm_r2, 'RMSE': lstm_rmse},\n",
    "    'XGBoost': {'RÂ²': xgb_r2, 'RMSE': xgb_rmse}\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(model_comparison).T\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# RÂ² comparison\n",
    "axes[0].bar(comparison_df.index, comparison_df['RÂ²'], color=['blue', 'green', 'orange'])\n",
    "axes[0].set_ylabel('RÂ² Score')\n",
    "axes[0].set_title('Model RÂ² Score Comparison')\n",
    "axes[0].set_ylim([0.9, 1.0])\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE comparison\n",
    "axes[1].bar(comparison_df.index, comparison_df['RMSE'], color=['blue', 'green', 'orange'])\n",
    "axes[1].set_ylabel('RMSE (Â°C)')\n",
    "axes[1].set_title('Model RMSE Comparison')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Machine Learning Model Performance Comparison', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Model Performance Summary:\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Create ensemble predictions (weighted average)\n",
    "# For demonstration, use simple average of XGBoost and NN\n",
    "ensemble_predictions = (nn_predictions + xgb_predictions[:len(nn_predictions)]) / 2\n",
    "ensemble_r2 = r2_score(y_test[:len(ensemble_predictions)], ensemble_predictions)\n",
    "ensemble_rmse = np.sqrt(mean_squared_error(y_test[:len(ensemble_predictions)], ensemble_predictions))\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Ensemble Model Performance:\")\n",
    "print(f\"  RÂ² Score: {ensemble_r2:.4f}\")\n",
    "print(f\"  RMSE: {ensemble_rmse:.4f}Â°C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Real-time Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeatIndexPredictor:\n",
    "    \"\"\"Real-time KMA Heat Index prediction pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, model, scaler, feature_cols):\n",
    "        self.model = model\n",
    "        self.scaler = scaler\n",
    "        self.feature_cols = feature_cols\n",
    "        \n",
    "    def preprocess(self, data):\n",
    "        \"\"\"Preprocess input data\"\"\"\n",
    "        # Ensure all features are present\n",
    "        for col in self.feature_cols:\n",
    "            if col not in data:\n",
    "                data[col] = 0\n",
    "        \n",
    "        # Select and order features\n",
    "        X = data[self.feature_cols].values.reshape(1, -1)\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        return X_scaled\n",
    "    \n",
    "    def predict(self, data):\n",
    "        \"\"\"Make prediction\"\"\"\n",
    "        X_scaled = self.preprocess(data)\n",
    "        prediction = self.model.predict(X_scaled, verbose=0)[0][0]\n",
    "        \n",
    "        # Determine heat stress category\n",
    "        if prediction < 25:\n",
    "            category = 'Comfortable'\n",
    "            color = 'green'\n",
    "        elif prediction < 28:\n",
    "            category = 'Caution'\n",
    "            color = 'yellow'\n",
    "        elif prediction < 31:\n",
    "            category = 'Extreme Caution'\n",
    "            color = 'orange'\n",
    "        elif prediction < 35:\n",
    "            category = 'Danger'\n",
    "            color = 'red'\n",
    "        else:\n",
    "            category = 'Extreme Danger'\n",
    "            color = 'darkred'\n",
    "        \n",
    "        return {\n",
    "            'heat_index': prediction,\n",
    "            'category': category,\n",
    "            'color': color\n",
    "        }\n",
    "    \n",
    "    def predict_batch(self, df):\n",
    "        \"\"\"Make batch predictions\"\"\"\n",
    "        predictions = []\n",
    "        for _, row in df.iterrows():\n",
    "            pred = self.predict(row.to_dict())\n",
    "            predictions.append(pred)\n",
    "        return predictions\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = HeatIndexPredictor(nn_model, scaler_X, feature_cols)\n",
    "\n",
    "# Test real-time prediction\n",
    "test_data = pd.DataFrame({\n",
    "    'temperature': [30, 35, 28],\n",
    "    'humidity': [70, 80, 60],\n",
    "    'wind_speed': [2, 1, 3],\n",
    "    'solar_radiation': [400, 500, 300],\n",
    "    'pressure': [1013, 1012, 1014],\n",
    "    'pm25': [25, 30, 20],\n",
    "    'pm10': [40, 45, 35],\n",
    "    'hour': [14, 15, 10],\n",
    "    'month': [7, 8, 6],\n",
    "    'is_weekend': [0, 0, 1]\n",
    "})\n",
    "\n",
    "print(\"ðŸ”® Real-time Prediction Examples:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, row in test_data.iterrows():\n",
    "    result = predictor.predict(row.to_dict())\n",
    "    print(f\"\\nScenario {i+1}:\")\n",
    "    print(f\"  Temperature: {row['temperature']}Â°C, Humidity: {row['humidity']}%\")\n",
    "    print(f\"  Predicted KMA Heat Index: {result['heat_index']:.2f}Â°C\")\n",
    "    print(f\"  Category: {result['category']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP values for XGBoost model interpretability\n",
    "import shap\n",
    "\n",
    "# Create SHAP explainer\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer.shap_values(X_test_xgb[:100])  # Use subset for visualization\n",
    "\n",
    "# Summary plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_test_xgb[:100], feature_names=xgb_features, show=False)\n",
    "plt.title('SHAP Feature Importance for KMA Heat Index Prediction')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature interaction plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.dependence_plot('temperature', shap_values, X_test_xgb[:100], \n",
    "                     feature_names=xgb_features, interaction_index='humidity', show=False)\n",
    "plt.title('Temperature vs Humidity Interaction Effect on Heat Index')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Models and Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "import joblib\n",
    "\n",
    "# Save neural network\n",
    "nn_model.save('../models/kma_heat_index_nn_model.h5')\n",
    "print(\"âœ… Neural Network model saved\")\n",
    "\n",
    "# Save LSTM\n",
    "lstm_model.save('../models/kma_heat_index_lstm_model.h5')\n",
    "print(\"âœ… LSTM model saved\")\n",
    "\n",
    "# Save XGBoost\n",
    "joblib.dump(xgb_model, '../models/kma_heat_index_xgb_model.pkl')\n",
    "print(\"âœ… XGBoost model saved\")\n",
    "\n",
    "# Save scalers\n",
    "joblib.dump(scaler_X, '../models/scaler_features.pkl')\n",
    "joblib.dump(scaler_lstm, '../models/scaler_lstm.pkl')\n",
    "print(\"âœ… Scalers saved\")\n",
    "\n",
    "# Generate report\n",
    "report = f\"\"\"\n",
    "MACHINE LEARNING ANALYSIS REPORT\n",
    "=================================\n",
    "\n",
    "MODELS EVALUATED:\n",
    "1. Deep Neural Network (4 layers)\n",
    "2. LSTM for Time Series\n",
    "3. XGBoost with Lag Features\n",
    "4. Ensemble Model\n",
    "\n",
    "PERFORMANCE SUMMARY:\n",
    "{comparison_df.round(4).to_string()}\n",
    "\n",
    "Ensemble Model:\n",
    "- RÂ² Score: {ensemble_r2:.4f}\n",
    "- RMSE: {ensemble_rmse:.4f}Â°C\n",
    "\n",
    "CLUSTERING ANALYSIS:\n",
    "- K-Means Silhouette Score: {kmeans_silhouette:.4f}\n",
    "- Identified {n_clusters} distinct weather patterns\n",
    "\n",
    "ANOMALY DETECTION:\n",
    "- Total anomalies: {len(anomalies)} ({len(anomalies)/len(X_cluster)*100:.2f}%)\n",
    "- Anomalies primarily occur at extreme heat index values\n",
    "\n",
    "KEY FINDINGS:\n",
    "1. XGBoost performs best with lag features\n",
    "2. Temperature is the most important predictor\n",
    "3. Lag features significantly improve predictions\n",
    "4. 5 distinct heatwave patterns identified\n",
    "5. Anomalies correspond to extreme heat events\n",
    "\n",
    "RECOMMENDATIONS:\n",
    "1. Deploy XGBoost for operational forecasting\n",
    "2. Use LSTM for long-term predictions\n",
    "3. Implement anomaly detection for early warnings\n",
    "4. Update models weekly with new data\n",
    "5. Monitor model drift and retrain as needed\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Save report\n",
    "with open('../reports/ml_analysis_report.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "print(\"\\nâœ… Report saved to ../reports/ml_analysis_report.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Assignment\n",
    "\n",
    "### Week 8 Tasks:\n",
    "\n",
    "1. **Deep Learning Models** (25 points)\n",
    "   - Implement and train neural networks\n",
    "   - Build LSTM for time series prediction\n",
    "   - Optimize hyperparameters\n",
    "\n",
    "2. **Advanced ML Techniques** (25 points)\n",
    "   - Apply XGBoost with feature engineering\n",
    "   - Create ensemble models\n",
    "   - Compare model performances\n",
    "\n",
    "3. **Clustering Analysis** (25 points)\n",
    "   - Identify heatwave patterns\n",
    "   - Compare clustering algorithms\n",
    "   - Analyze cluster characteristics\n",
    "\n",
    "4. **Anomaly Detection** (25 points)\n",
    "   - Detect extreme heat events\n",
    "   - Implement early warning system\n",
    "   - Validate with historical events\n",
    "\n",
    "### Bonus Challenge:\n",
    "- Implement transformer model for heat index prediction\n",
    "- Create AutoML pipeline for model selection\n",
    "- Develop mobile app for real-time predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this week, we covered:\n",
    "- âœ… Deep learning with neural networks and LSTM\n",
    "- âœ… XGBoost for advanced predictions\n",
    "- âœ… Clustering for pattern discovery\n",
    "- âœ… Anomaly detection for extreme events\n",
    "- âœ… Model interpretability with SHAP\n",
    "\n",
    "### Next Week Preview:\n",
    "**Week 9: Advanced Visualization and Dashboards**\n",
    "- Interactive dashboards with Plotly/Dash\n",
    "- Real-time monitoring systems\n",
    "- Geographic visualizations\n",
    "- Report generation\n",
    "\n",
    "### Resources:\n",
    "- [TensorFlow Documentation](https://www.tensorflow.org/)\n",
    "- [XGBoost Documentation](https://xgboost.readthedocs.io/)\n",
    "- [SHAP Documentation](https://shap.readthedocs.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**End of Week 8**\n",
    "\n",
    "*Instructor: Sohn Chul*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}